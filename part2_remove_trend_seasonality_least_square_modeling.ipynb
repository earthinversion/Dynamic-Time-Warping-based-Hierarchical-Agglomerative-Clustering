{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "going-illinois",
   "metadata": {},
   "source": [
    "## PART 2: Remove trend and seasonality by least-squares modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-vessel",
   "metadata": {},
   "source": [
    "This script aims to:\n",
    "- Least squares model to remove the tidal and seasonal signals as well as co-seismic jumps\n",
    "- Earthquakes selected are obtained from wilber 3 web interface\n",
    "- Co-seismic jumps from selected earthquakes (>5.1 Mw) \n",
    "\n",
    "### Required:\n",
    "- python\n",
    "- pandas\n",
    "- jupyter\n",
    "- notebook\n",
    "- matplotlib\n",
    "- seaborn\n",
    "\n",
    "this should be easy to set up in a conda env: conda create -c conda-forge -n dtwclustering python=3.7 pandas numpy jupyter notebook matplotlib seaborn\n",
    "\n",
    "__Author: Utpal Kumar @Institute of Earth Sciences, Academia Sinica__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-syntax",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "disabled-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import least_squares as ls\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from dtwclustering.analysis_support import toYearFraction as tyf\n",
    "from dtwclustering.leastSquareModeling import lsqmodeling\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import style\n",
    "import scipy.io as sio\n",
    "import os, glob\n",
    "from IPython.display import Image, display\n",
    "from random import randrange "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exclusive-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "## default matplotlib parameters\n",
    "import matplotlib\n",
    "font = {'family' : 'Times',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-delhi",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nonprofit-humanity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GUKN_U</th>\n",
       "      <th>SANL_U</th>\n",
       "      <th>GS26_U</th>\n",
       "      <th>GS32_U</th>\n",
       "      <th>GS33_U</th>\n",
       "      <th>GS27_U</th>\n",
       "      <th>NAAO_U</th>\n",
       "      <th>PEPU_U</th>\n",
       "      <th>SJPU_U</th>\n",
       "      <th>GS31_U</th>\n",
       "      <th>...</th>\n",
       "      <th>GS15_U</th>\n",
       "      <th>GS28_U</th>\n",
       "      <th>TTUN_U</th>\n",
       "      <th>DNAN_U</th>\n",
       "      <th>MESN_U</th>\n",
       "      <th>W029_U</th>\n",
       "      <th>GS16_U</th>\n",
       "      <th>WULU_U</th>\n",
       "      <th>NSHE_U</th>\n",
       "      <th>DOSH_U</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-01</th>\n",
       "      <td>-3.6104</td>\n",
       "      <td>-21.535</td>\n",
       "      <td>-11.6553</td>\n",
       "      <td>-4.5357</td>\n",
       "      <td>2.7128</td>\n",
       "      <td>6.5048</td>\n",
       "      <td>15.7129</td>\n",
       "      <td>10.9418</td>\n",
       "      <td>-30.4328</td>\n",
       "      <td>8.5945</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.2284</td>\n",
       "      <td>11.2566</td>\n",
       "      <td>30.0541</td>\n",
       "      <td>75.6983</td>\n",
       "      <td>-4.4475</td>\n",
       "      <td>-4.054</td>\n",
       "      <td>-6.0754</td>\n",
       "      <td>-60.0987</td>\n",
       "      <td>4.3838</td>\n",
       "      <td>-22.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-02</th>\n",
       "      <td>4.3376</td>\n",
       "      <td>-19.425</td>\n",
       "      <td>-5.1123</td>\n",
       "      <td>-3.3467</td>\n",
       "      <td>6.4358</td>\n",
       "      <td>11.1978</td>\n",
       "      <td>35.5059</td>\n",
       "      <td>6.8598</td>\n",
       "      <td>-27.1238</td>\n",
       "      <td>7.3335</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.4244</td>\n",
       "      <td>19.1496</td>\n",
       "      <td>29.7561</td>\n",
       "      <td>79.6213</td>\n",
       "      <td>-15.8095</td>\n",
       "      <td>-6.475</td>\n",
       "      <td>-12.6834</td>\n",
       "      <td>-49.5617</td>\n",
       "      <td>10.9828</td>\n",
       "      <td>-11.7836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>13.0606</td>\n",
       "      <td>-15.892</td>\n",
       "      <td>-2.9963</td>\n",
       "      <td>-2.4947</td>\n",
       "      <td>7.1608</td>\n",
       "      <td>7.1998</td>\n",
       "      <td>23.5059</td>\n",
       "      <td>8.8988</td>\n",
       "      <td>-14.8608</td>\n",
       "      <td>7.1765</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.2764</td>\n",
       "      <td>25.5186</td>\n",
       "      <td>34.5391</td>\n",
       "      <td>81.3833</td>\n",
       "      <td>-13.0965</td>\n",
       "      <td>9.169</td>\n",
       "      <td>-4.4514</td>\n",
       "      <td>-53.3637</td>\n",
       "      <td>5.0418</td>\n",
       "      <td>-25.0546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>0.3386</td>\n",
       "      <td>-23.530</td>\n",
       "      <td>-6.6133</td>\n",
       "      <td>-9.6667</td>\n",
       "      <td>2.1708</td>\n",
       "      <td>10.9468</td>\n",
       "      <td>27.1809</td>\n",
       "      <td>17.0198</td>\n",
       "      <td>-36.1068</td>\n",
       "      <td>4.6815</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.8944</td>\n",
       "      <td>12.7456</td>\n",
       "      <td>23.0081</td>\n",
       "      <td>75.2663</td>\n",
       "      <td>-21.1925</td>\n",
       "      <td>-12.239</td>\n",
       "      <td>-4.4514</td>\n",
       "      <td>-61.2687</td>\n",
       "      <td>1.3368</td>\n",
       "      <td>-21.9806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>-3.1744</td>\n",
       "      <td>-21.213</td>\n",
       "      <td>-13.0963</td>\n",
       "      <td>-10.5357</td>\n",
       "      <td>-0.1872</td>\n",
       "      <td>6.2038</td>\n",
       "      <td>21.3349</td>\n",
       "      <td>13.4428</td>\n",
       "      <td>-29.7658</td>\n",
       "      <td>4.0915</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.9304</td>\n",
       "      <td>9.0336</td>\n",
       "      <td>31.5471</td>\n",
       "      <td>76.9883</td>\n",
       "      <td>-40.2215</td>\n",
       "      <td>-9.293</td>\n",
       "      <td>0.2166</td>\n",
       "      <td>-85.4437</td>\n",
       "      <td>0.2348</td>\n",
       "      <td>-20.8926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             GUKN_U  SANL_U   GS26_U   GS32_U  GS33_U   GS27_U   NAAO_U  \\\n",
       "Time                                                                      \n",
       "2007-01-01  -3.6104 -21.535 -11.6553  -4.5357  2.7128   6.5048  15.7129   \n",
       "2007-01-02   4.3376 -19.425  -5.1123  -3.3467  6.4358  11.1978  35.5059   \n",
       "2007-01-03  13.0606 -15.892  -2.9963  -2.4947  7.1608   7.1998  23.5059   \n",
       "2007-01-04   0.3386 -23.530  -6.6133  -9.6667  2.1708  10.9468  27.1809   \n",
       "2007-01-05  -3.1744 -21.213 -13.0963 -10.5357 -0.1872   6.2038  21.3349   \n",
       "\n",
       "             PEPU_U   SJPU_U  GS31_U  ...   GS15_U   GS28_U   TTUN_U   DNAN_U  \\\n",
       "Time                                  ...                                       \n",
       "2007-01-01  10.9418 -30.4328  8.5945  ... -10.2284  11.2566  30.0541  75.6983   \n",
       "2007-01-02   6.8598 -27.1238  7.3335  ...  -5.4244  19.1496  29.7561  79.6213   \n",
       "2007-01-03   8.8988 -14.8608  7.1765  ...  -3.2764  25.5186  34.5391  81.3833   \n",
       "2007-01-04  17.0198 -36.1068  4.6815  ... -10.8944  12.7456  23.0081  75.2663   \n",
       "2007-01-05  13.4428 -29.7658  4.0915  ...  -5.9304   9.0336  31.5471  76.9883   \n",
       "\n",
       "             MESN_U  W029_U   GS16_U   WULU_U   NSHE_U   DOSH_U  \n",
       "Time                                                             \n",
       "2007-01-01  -4.4475  -4.054  -6.0754 -60.0987   4.3838 -22.1776  \n",
       "2007-01-02 -15.8095  -6.475 -12.6834 -49.5617  10.9828 -11.7836  \n",
       "2007-01-03 -13.0965   9.169  -4.4514 -53.3637   5.0418 -25.0546  \n",
       "2007-01-04 -21.1925 -12.239  -4.4514 -61.2687   1.3368 -21.9806  \n",
       "2007-01-05 -40.2215  -9.293   0.2166 -85.4437   0.2348 -20.8926  \n",
       "\n",
       "[5 rows x 115 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load pickle data\n",
    "dataloc = \"pickleFiles\"\n",
    "dUU=pd.read_pickle(os.path.join(dataloc,\"dU_data.pickle\"))\n",
    "dNN=pd.read_pickle(os.path.join(dataloc,\"dN_data.pickle\"))\n",
    "dEE=pd.read_pickle(os.path.join(dataloc,\"dE_data.pickle\"))\n",
    "dUU.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-seventh",
   "metadata": {},
   "source": [
    "##  Least square modeling for all the stations for three-components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-crazy",
   "metadata": {},
   "source": [
    "### Remove seasonlity only (keep the trend and co-seismic jumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "endless-senate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished LSQ (U): WULU: : 115it [01:37, 10.58it/s]\n",
      "Finished LSQ (N): SHJU: : 115it [01:54,  1.00it/s]\n",
      "Finished LSQ (E): GS16: : 115it [02:08,  8.70it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(dataloc,\"dU_wo_seasn.pickle\")) or not os.path.exists(os.path.join(dataloc,\"dN_wo_seasn.pickle\")) or not os.path.exists(os.path.join(dataloc,\"dE_wo_seasn.pickle\")):\n",
    "    final_dU, final_dN, final_dE = lsqmodeling(dUU, dNN, dEE,stnlocfile=\"helper_files/stn_loc.txt\",  plot_results=False, remove_trend=False, remove_seasonality=True, remove_jumps=False)\n",
    "else:\n",
    "    final_dU=pd.read_pickle(os.path.join(dataloc,\"dU_wo_seasn.pickle\"))\n",
    "    final_dN=pd.read_pickle(os.path.join(dataloc,\"dN_wo_seasn.pickle\"))\n",
    "    final_dE=pd.read_pickle(os.path.join(dataloc,\"dE_wo_seasn.pickle\"))\n",
    "    final_dU.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-verse",
   "metadata": {},
   "source": [
    "## Check the least square modeling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "addressed-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_U_png = glob.glob(os.path.join(\"LSQOut\",\"*_U.png\"))\n",
    "all_N_png = glob.glob(os.path.join(\"LSQOut\",\"*_N.png\"))\n",
    "all_E_png = glob.glob(os.path.join(\"LSQOut\",\"*_E.png\"))\n",
    "\n",
    "\n",
    "if all_U_png:\n",
    "    display(Image(filename=all_U_png[randrange(0, len(all_U_png))]))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "arranged-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_N_png:\n",
    "    display(Image(filename=all_N_png[randrange(0, len(all_N_png))])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "better-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_E_png:\n",
    "    display(Image(filename=all_E_png[randrange(0, len(all_E_png))])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-assault",
   "metadata": {},
   "source": [
    "## Save lsq modeled result in pickle and mat format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adolescent-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(dataloc,\"dU_wo_seasn.pickle\")) or not os.path.exists(os.path.join(dataloc,\"dN_wo_seasn.pickle\")) or not os.path.exists(os.path.join(dataloc,\"dE_wo_seasn.pickle\")):\n",
    "\n",
    "    final_dU.to_pickle(os.path.join(dataloc,\"dU_wo_seasn.pickle\"))\n",
    "    final_dN.to_pickle(os.path.join(dataloc,\"dN_wo_seasn.pickle\"))\n",
    "    final_dE.to_pickle(os.path.join(dataloc,\"dE_wo_seasn.pickle\"))\n",
    "\n",
    "# read station information\n",
    "station_loc_file=\"helper_files/stn_loc.txt\"\n",
    "stnloc = pd.read_csv(station_loc_file, header=None,\n",
    "                          sep='\\s+', names=['stn', 'lon', 'lat'])\n",
    "stnloc.set_index('stn', inplace=True)\n",
    "new_stn_info_list = []\n",
    "for stn in final_dU.columns.values:\n",
    "    stn = stn.split(\"_\")[0]\n",
    "    _dict={}\n",
    "    _dict['stn'] = stn\n",
    "    _dict['lon'] = stnloc.loc[stn,'lon']\n",
    "    _dict['lat'] = stnloc.loc[stn,'lat']\n",
    "    new_stn_info_list.append(_dict)\n",
    "    \n",
    "stn_info_df = pd.DataFrame(new_stn_info_list)\n",
    "stn_info_df.head()\n",
    "if not os.path.exists(os.path.join(dataloc,'all_data_wo_seasn.mat')):\n",
    "    sio.savemat(os.path.join(dataloc,'all_data_wo_seasn.mat'), {'slat': stn_info_df['lat'].values, 'slon': stn_info_df['lon'].values, 'tdata': final_dU.index.values,\n",
    "                                        'stns': stn_info_df['stn'].values, 'dU': np.array(final_dU), 'dN': np.array(final_dN), 'dE': np.array(final_dE)})\n",
    "\n",
    "# # Saving the slope info in the mat file\n",
    "# stn_slope = pd.read_csv('stn_slope_res_U.txt', sep='\\s+',\n",
    "#                         header=None, names=['stn', 'lon', 'lat', 'slope'])\n",
    "# sio.savemat('slope_info.mat', {'slat': stn_slope['lat'].values, 'slon': stn_slope['lon'].values,\n",
    "#                                'stns': stn_slope['stn'].values, 'slope': stn_slope['slope'].values})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-editor",
   "metadata": {},
   "source": [
    "### Compute dN, dE and dU without seasonality, trend and jumps\n",
    "- to compute the CME (common mode error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "russian-wisconsin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished LSQ (U): HUWE: : 115it [01:43,  1.11it/s]\n",
      "Finished LSQ (N): SHJU: : 115it [02:00, 12.34it/s]\n",
      "Finished LSQ (E): GS16: : 115it [02:12, 11.34it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(dataloc,\"dU_wo_seasn_trend_jump.pickle\")) or not os.path.exists(os.path.join(dataloc,\"dN_wo_seasn_trend_jump.pickle\")) or not os.path.exists(os.path.join(dataloc,\"dE_wo_seasn_trend_jump.pickle\")):\n",
    "    final_dU_2, final_dN_2, final_dE_2 = lsqmodeling(dUU, dNN, dEE,stnlocfile=\"helper_files/stn_loc.txt\",  plot_results=False, remove_trend=True, remove_seasonality=True, remove_jumps=True)\n",
    "else:\n",
    "    final_dU_2=pd.read_pickle(os.path.join(dataloc,\"dU_wo_seasn_trend_jump.pickle\"))\n",
    "    final_dN_2=pd.read_pickle(os.path.join(dataloc,\"dN_wo_seasn_trend_jump.pickle\"))\n",
    "    final_dE_2=pd.read_pickle(os.path.join(dataloc,\"dE_wo_seasn_trend_jump.pickle\"))\n",
    "    final_dU_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sought-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(dataloc,\"dU_wo_seasn_trend_jump.pickle\")) or not os.path.exists(os.path.join(dataloc,\"dN_wo_seasn_trend_jump.pickle\")) or not os.path.exists(os.path.join(dataloc,\"dE_wo_seasn_trend_jump.pickle\")):\n",
    "    final_dU_2.to_pickle(os.path.join(dataloc,\"dU_wo_seasn_trend_jump.pickle\"))\n",
    "    final_dN_2.to_pickle(os.path.join(dataloc,\"dN_wo_seasn_trend_jump.pickle\"))\n",
    "    final_dE_2.to_pickle(os.path.join(dataloc,\"dE_wo_seasn_trend_jump.pickle\"))\n",
    "\n",
    "# read station information\n",
    "station_loc_file=\"helper_files/stn_loc.txt\"\n",
    "stnloc = pd.read_csv(station_loc_file, header=None,\n",
    "                          sep='\\s+', names=['stn', 'lon', 'lat'])\n",
    "stnloc.set_index('stn', inplace=True)\n",
    "new_stn_info_list = []\n",
    "\n",
    "# east_coast_stations = ['PEPU', 'DAJN', 'NDHU', 'CHUN', 'SHUL', 'TUNH', 'DAWU', 'CHGO', 'YENL', 'SHAN', 'SOFN', 'TAPE', 'ERPN', 'CHEN', 'TAPO', 'SINL', 'LONT', 'JULI', 'JSUI', 'TTUN', 'NAAO', 'SPAO', 'MOTN', 'SLNP', 'WARO', 'SLIN', 'WULU']\n",
    "for stn in final_dU_2.columns.values:\n",
    "    stn = stn.split(\"_\")[0]\n",
    "    \n",
    "    _dict={}\n",
    "    _dict['stn'] = stn\n",
    "    _dict['lon'] = stnloc.loc[stn,'lon']\n",
    "    _dict['lat'] = stnloc.loc[stn,'lat']\n",
    "    new_stn_info_list.append(_dict)\n",
    "    \n",
    "stn_info_df = pd.DataFrame(new_stn_info_list)\n",
    "stn_info_df.head()\n",
    "stn_info_df.to_csv('helper_files/selected_stations_info.txt', index=False)\n",
    "if not os.path.exists(os.path.join(dataloc,'all_data_wo_seasn_trend_jump.mat')):\n",
    "    sio.savemat(os.path.join(dataloc,'all_data_wo_seasn_trend_jump.mat'), {'slat': stn_info_df['lat'].values, 'slon': stn_info_df['lon'].values, 'tdata': final_dU_2.index.values,\n",
    "                                        'stns': stn_info_df['stn'].values, 'dU': np.array(final_dU_2), 'dN': np.array(final_dN_2), 'dE': np.array(final_dE_2)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-swift",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
