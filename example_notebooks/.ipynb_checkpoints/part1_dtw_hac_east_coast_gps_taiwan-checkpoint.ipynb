{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optical-commons",
   "metadata": {},
   "source": [
    "## PART 1: Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-george",
   "metadata": {},
   "source": [
    "### Required:\n",
    "- python\n",
    "- pandas\n",
    "- jupyter\n",
    "- notebook\n",
    "- matplotlib\n",
    "- seaborn\n",
    "\n",
    "this should be easy to set up in a conda env: conda create -c conda-forge -n dtwclustering python=3.7 pandas numpy jupyter notebook matplotlib seaborn\n",
    "\n",
    "__Author: Utpal Kumar @Institute of Earth Sciences, Academia Sinica__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-corporation",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "premier-amino",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dtwhaclustering'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-95d0109cbad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdtwhaclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis_support\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdec2dt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdtwclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis_support\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtoYearFraction\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtyf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dtwhaclustering'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dtwhaclustering.analysis_support import dec2dt\n",
    "from functools import reduce\n",
    "from dtwclustering.analysis_support import toYearFraction as tyf\n",
    "import scipy.io as sio\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "## default matplotlib parameters\n",
    "import matplotlib\n",
    "font = {'family' : 'Times',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-identification",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource=\"TimeSeriesReleased1993.01.01_2018.04.30/\" #data is stored in this directory\n",
    "all_data_files=glob.glob(datasource+\"*.COR\") ##all data file names\n",
    "print(\"Total station data to begin with: \", len(all_data_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-merchandise",
   "metadata": {},
   "source": [
    "Read COR files to build start and end times of all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-requirement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## extract the start time, end time and number of points in the time series for each stations\n",
    "datalength_list = []\n",
    "for dfile in all_data_files:\n",
    "    _mydict = {}\n",
    "    df=pd.read_csv(dfile,header=None,sep='\\s+')\n",
    "    stn=dfile.split(\"/\")[1].split(\".\")[0]\n",
    "\n",
    "    stime=df.iloc[:,0].min()\n",
    "    etime=df.iloc[:,0].max()\n",
    "\n",
    "    tdataPoints=df.shape[0]\n",
    "    _mydict['stn'] = stn\n",
    "    _mydict['stime'] = stime\n",
    "    _mydict['etime'] = etime\n",
    "    _mydict['tdataPoints'] = tdataPoints\n",
    "    datalength_list.append(_mydict)\n",
    "    \n",
    "datalength = pd.DataFrame(datalength_list)\n",
    "datalength.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-workstation",
   "metadata": {},
   "source": [
    "### Histogram of the data availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,1,sharex=True)\n",
    "sns.distplot(datalength['stime'].values, hist=True, kde=False, bins='auto', color = 'darkblue', hist_kws={'edgecolor':'black', \"label\": \"Start Time\"},ax=ax[0])\n",
    "ax[0].legend()\n",
    "sns.distplot(datalength['etime'].values, hist=True, kde=False, bins=10, color = 'darkred', hist_kws={'edgecolor':'black', \"label\": \"End Time\"},ax=ax[1])\n",
    "ax[1].set_xlabel('Years')\n",
    "ax[1].legend()\n",
    "plt.xlim(datalength['stime'].min(), datalength['etime'].max())\n",
    "# plt.savefig('s_e_timeHistogram.png',bbox_inches='tight')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-protocol",
   "metadata": {},
   "source": [
    "### Select the data files between 2007-2018 and npts=4000 [360*11 = 3960] days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = 2007\n",
    "endtime = 2018\n",
    "selData=datalength[(datalength['stime']<starttime) & (datalength['etime']>endtime) & (datalength['tdataPoints']>4000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "## New Selected Data\n",
    "selstns_all=selData['stn'].values\n",
    "print(\"Number of stations selected: \",len(selstns_all))\n",
    "\n",
    "## Writing all selected data into a data frame\n",
    "main_dU=[]\n",
    "main_dN=[]\n",
    "main_dE=[]\n",
    "for s1 in selstns_all:\n",
    "    duu='{}_U'.format(s1)\n",
    "    dnn='{}_N'.format(s1)\n",
    "    dee='{}_E'.format(s1)\n",
    "    selGroundMotion=pd.read_csv(os.path.join(datasource,s1+'.COR'),header=None,delimiter=r'\\s+',names=['year','lat','lon','hgt','dN','dE','dU','FLAG(reserved)'])\n",
    "    timeVal=dec2dt(selGroundMotion.values[:,0])\n",
    "    selGroundMotion[\"Time\"]=timeVal\n",
    "    selGroundMotion.set_index(\"Time\",inplace=True)\n",
    "    \n",
    "    # Extracting data between start and end time and renaming the columns\n",
    "    df2=selGroundMotion.loc[(selGroundMotion.year>starttime) & (selGroundMotion.year<endtime),['dN','dE','dU']].rename(columns={'dN':dnn,'dE':dee,'dU':duu})\n",
    "    # Removing the 2-sigma outliers\n",
    "    df2=df2[(np.abs(df2[dnn]-df2[dnn].mean())<=2*df2[dnn].std()) | (np.abs(df2[dee]-df2[dee].mean())<=2*df2[dee].std()) | (np.abs(df2[duu]-df2[duu].mean())<=2*df2[duu].std())]\n",
    "\n",
    "\n",
    "    # # # Resampling the data for each day and interpolating for unavailable entries\n",
    "    df3=df2.resample('D').last().interpolate(method='nearest')\n",
    "    # df3=df2 #no interpolation\n",
    "    # Storing each station data in a single list separately for dN, dE and dU\n",
    "    main_dN.append(df3[dnn])\n",
    "    main_dE.append(df3[dee])\n",
    "    main_dU.append(df3[duu])\n",
    "\n",
    "# Concatenating all the data frames in the list to make a single data frame\n",
    "dNN=reduce(lambda x, y: pd.concat([x, y],axis=1), main_dN)\n",
    "dEE=reduce(lambda x, y: pd.concat([x, y],axis=1), main_dE)\n",
    "dUU=reduce(lambda x, y: pd.concat([x, y],axis=1), main_dU)\n",
    "\n",
    "## Remove stations with missing data in the beginning or end\n",
    "allcols=dUU.columns.values\n",
    "cols_remU=[]\n",
    "for i in range(len(allcols)):\n",
    "    #check first and last row\n",
    "    if np.isnan(dUU.iloc[0,i]) or np.isnan(dUU.iloc[-1,i]):\n",
    "        cols_remU.append(allcols[i])\n",
    "\n",
    "allcolsE=dEE.columns.values\n",
    "cols_remE=[]\n",
    "for i in range(len(allcolsE)):\n",
    "    if np.isnan(dEE.iloc[0,i]) or np.isnan(dEE.iloc[-1,i]):\n",
    "        cols_remE.append(allcolsE[i])\n",
    "\n",
    "allcolsN=dNN.columns.values\n",
    "cols_remN=[]\n",
    "for i in range(len(allcolsN)):\n",
    "    if np.isnan(dNN.iloc[0,i]) or np.isnan(dNN.iloc[-1,i]):\n",
    "        cols_remN.append(allcolsN[i])\n",
    "\n",
    "\n",
    "dUU=dUU.drop(cols_remU, axis=1)\n",
    "dNN=dNN.drop(cols_remN, axis=1)\n",
    "dEE=dEE.drop(cols_remE, axis=1)\n",
    "dNN.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-database",
   "metadata": {},
   "source": [
    "### Save into pickle file and mat file (MATLAB purpose) for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = \"pickleFiles\"\n",
    "os.makedirs(selected_data, exist_ok=True) #don't make if already exists\n",
    "\n",
    "dUU.to_pickle(os.path.join(selected_data,\"dU_data.pickle\"))\n",
    "dNN.to_pickle(os.path.join(selected_data,\"dN_data.pickle\"))\n",
    "dEE.to_pickle(os.path.join(selected_data,\"dE_data.pickle\"))\n",
    "\n",
    "# ## create new column of \"year\" with decimal year values instead of string\n",
    "# year = []\n",
    "# for dd in dUU.index:\n",
    "#     year.append(round(tyf(dd), 5))\n",
    "# dUU['year'] = year\n",
    "# dNN['year'] = year\n",
    "# dEE['year'] = year\n",
    "\n",
    "\n",
    "# # Save into mat file\n",
    "# sio.savemat(os.path.join(selected_data,'dU_data.mat'), {name: col.values for name, col in dUU.items()})\n",
    "# sio.savemat(os.path.join(selected_data,'dN_data.mat'), {name: col.values for name, col in dNN.items()})\n",
    "# sio.savemat(os.path.join(selected_data,'dE_data.mat'), {name: col.values for name, col in dEE.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-express",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
